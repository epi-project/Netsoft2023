{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>The graph initialisation</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<h1>The graph initialisation</h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = {\"cluster1-cntx\":[\"cluster1-cntx\",\"cluster2-cntx\"], \"cluster2-cntx\":[\"cluster2-cntx\",\"cluster1-cntx\"]}#, \"cluster3-cntx\":[\"cluster3-cntx\"]}\n",
    "#Change the graph according to the current cluster configurations, so if you remove cluster 3 from the dict value of cluster 1, means that cluster 1 is not connected to 3\n",
    "\n",
    "#Make sure the latency graph matches the G graph \"who can talk to whom? Then what is the latench?\n",
    "link_latency = {\"cluster1-cntx\":{\"cluster1-cntx\":0, \"cluster2-cntx\":10},\"cluster2-cntx\":{\"cluster2-cntx\":0, \"cluster1-cntx\":10}}\n",
    "\n",
    "#Change this and initialise to reflect the simulated latency on each cluster-to-cluster link\n",
    "bf = {\"firewall\":{\"firewalls\":10, \"firewallm\":5, \"firewalll\":1}, \"encrypt\":{\"encrypts\":10, \"encryptm\":5, \"encryptl\":1}, \"decrypt\":{\"decrypts\":10, \"decryptm\":5, \"decryptl\":1}}\n",
    "#This reflects that the processing power of firewalll is the best in terms of processing delay and the worst with smaller configurations\n",
    "\n",
    "G_req = {\"firewall\":[\"encrypt\"], \"encrypt\":[\"firewall\"]}#, \"decrypt\":[\"firewall\"]}\n",
    "#This is the actual request rule that you want to be placed. Change this with each experiment to reflect the rule needed\n",
    "#MAKE SURE THAT THE REQUEST G_REQ MATCHES THE U VALUE\n",
    "\n",
    "usecase = {\"EHR:1C:E-F-D\":{\"firewall\":30,\"encrypt\": 20, \"decrypt\":10, \"SLA\":100}, \"EHR:1C:F-E-D\":{\"firewall\":40, \"encrypt\": 10, \"decrypt\": 10, \"SLA\": 100}, \"EHR:1C:E-D\":{\"encrypt\": 20, \"decrypt\":20, \"SLA\":100}, \"EHR:1C:E-F\":{\"firewall\":30,\"encrypt\": 20, \"SLA\":100}, \"EHR:1C:E\":{\"encrypt\": 50, \"SLA\":100}, \"EHR:1C:F\":{\"firewall\":20, \"SLA\":100}, \"EHR:10C:E-F-D\":{\"firewall\":130,\"encrypt\": 80, \"decrypt\":20, \"SLA\":100}, \"EHR:10C:E\":{\"encrypt\": 80, \"SLA\":100}, \"EHR:10C:F-E-D\":{\"firewall\":160, \"encrypt\": 20, \"decrypt\":20, \"SLA\":100},\"EHR:10C:E-D\":{\"encrypt\": 80, \"decrypt\":80, \"SLA\":100}, \"EHR:10C:E-F\":{\"firewall\":130,\"encrypt\": 80, \"SLA\":100}, \"EHR:10C:F\":{\"firewall\":100,\"SLA\":100}, \"STREAM:1C:E-F-D\":{\"firewall\":130,\"encrypt\": 80, \"decrypt\":25, \"SLA\":10}, \"STREAM:1C:F-E-D\":{\"firewall\":180,\"encrypt\": 25, \"decrypt\": 25, \"SLA\": 10}, \"STREAM:1C:E-D\":{\"encrypt\": 80, \"decrypt\":80, \"SLA\":10}, \"STREAM:1C:E-F\":{\"firewall\":130,\"encrypt\": 80, \"SLA\":10}, \"STREAM:1C:F\":{\"firewall\":180, \"SLA\":10}, \"STREAM:10C:E-F-D\":{\"firewall\":300,\"encrypt\": 150, \"decrypt\":40, \"SLA\":10}, \"STREAM:10C:F-E-D\":{\"firewall\":350, \"encrypt\": 40, \"decrypt\":40, \"SLA\":10},\"STREAM:10C:E-D\":{\"encrypt\": 150, \"decrypt\":150, \"SLA\":10}, \"STREAM:10C:E-F\":{\"encrypt\":300,\"firewall\": 300, \"SLA\":10},  \"ALGO:1C:E-F-D\":{\"firewall\":30,\"encrypt\": 20, \"decrypt\":10, \"SLA\":100}, \"ALGO:1C:F-E-D\":{\"firewall\":40,\"encrypt\": 10, \"decrypt\": 10, \"SLA\": 100}, \"ALGO:1C:E-D\":{\"encrypt\": 20, \"decrypt\":20, \"SLA\":100}, \"ALGO:1C:E-F\":{\"firewall\":30,\"encrypt\": 20, \"SLA\":100}, \"ALGO:1C:F\":{\"firewall\":40, \"SLA\":100}, \"ALGO:10C:E-F-D\":{\"firewall\":130,\"encrypt\": 80, \"decrypt\":20, \"SLA\":100}, \"ALGO:10C:F-E-D\":{\"firewall\":160, \"encrypt\": 20, \"decrypt\":20, \"SLA\":100},\"ALGO:10C:E-D\":{\"encrypt\": 80, \"decrypt\":80, \"SLA\":100}, \"ALGO:10C:E-F\":{\"firewall\":130,\"encrypt\": 80, \"SLA\":100}, \"ALGO:10C:F\":{\"firewall\":160,\"SLA\":100}, \"EHR:50C:F\":{\"firewall\":270, \"SLA\":100}, \"EHR:50C:E\":{\"encrypt\": 130, \"SLA\":100},\"EHR:20C:F\":{\"firewall\":200, \"SLA\":100}, \"EHR:20C:E\":{\"encrypt\":70, \"SLA\":100}, \"EHR:30C:F\":{\"firewall\":220, \"SLA\":100}, \"EHR:30C:E\":{\"encrypt\":100, \"SLA\":100}, \"EHR:40C:F\":{\"firewall\":250, \"SLA\":100}, \"EHR:40C:E\":{\"encrypt\":120, \"SLA\":100}, \"STREAM:20C:F\":{\"firewall\":750, \"SLA\":10}, \"STREAM:20C:E-F\":{\"firewall\":750, \"encrypt\":500, \"SLA\":10},\"STREAM:20C:E\":{\"encrypt\":500, \"SLA\":10}}\n",
    "#Do not change this, these are the profiled values of CPU average running different use cases and different rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To generate different usecases under different requests \n",
    "\n",
    "G_req1 = {\"encrypt\":[\"firewall\"], \"firewall\":[\"encrypt\", \"decrypt\"], \"decrypt\":[\"firewall\"]}\n",
    "G_req2 = {\"firewall\":[\"encrypt\"], \"encrypt\":[\"firewall\", \"decrypt\"], \"decrypt\":[\"encrypt\"]}\n",
    "G_req3 = {\"encrypt\":[\"decrypt\"], \"decrypt\":[\"encrypt\"]}\n",
    "G_req4 = {\"encrypt\":[\"firewall\"], \"firewall\":[\"encrypt\"]}\n",
    "G_req5 = {\"firewall\":[\"firewall\"]}\n",
    "G_req6 = {\"encrypt\":[\"encrypt\"]}\n",
    "\n",
    "#Specify by initialising G_req = G_reqX\n",
    "#You also need specify use case by initialising value u as an example, u= \"EHR:1C:E-F-D\"\n",
    "#Key values starting with EHR refers to usecase A\n",
    "#Keys starting with STREAM use case B\n",
    "#Keys starting with ALGO refers to usecase C\n",
    "#Keys represeted as \"USECASE:NUMBEROFCLIENTS:RULE\"\n",
    "#Make sure while initialising a use case to match it to the requested chain\n",
    "#Ex: \n",
    "u = \"EHR:10C:E-F-D\"\n",
    "G_req=G_req1\n",
    "#We have one proxy so leave this proxy value as it is, unless the proxy service you have has a different name\n",
    "proxy=\"proxy1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>The graph illustration</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<h1>The graph illustration</h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "def draw(G):\n",
    "    infra = nx.DiGraph()\n",
    " #   for cl in range(len(G)):\n",
    " #       i = list(G.keys())[cl]\n",
    " #       infra.add_node(i)\n",
    "    for cl in range(len(G)):\n",
    "        i = list(G.keys())[cl]\n",
    "        for link in range(len(G[i])):\n",
    "            infra.add_edges_from([(i, G[i][link])], weight=link_latency[i][G[i][link]])\n",
    "    edge_labels=dict([((u,v,),d['weight'])\n",
    "                 for u,v,d in infra.edges(data=True)])\n",
    "\n",
    "\n",
    "    pos = nx.spring_layout(infra)\n",
    "    nx.draw_networkx_edge_labels(infra,pos,edge_labels=edge_labels)\n",
    "    nx.draw_networkx_labels(infra, pos)\n",
    "    #plt.figure(figsize=(8,8))\n",
    "    \n",
    "    nx.draw(infra, pos=pos, with_labels=True)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The number of running pods of a microservice on each cluster\n",
    "import os\n",
    "def runningPods(cluster, app_name):\n",
    "    output = os.popen('sudo kubectl top pod --context=' + cluster).read()\n",
    "    lines = output.split(\"\\n\")\n",
    "    numPods = 0\n",
    "    which_app = len(app_name)\n",
    "    for line in lines[:-1]:\n",
    "        items = line.split()\n",
    "        if len(items[0]) > 8 and items[0][:which_app] == app_name:\n",
    "            numPods += 1\n",
    "    return numPods\n",
    "runningPods(\"cluster1-cntx\",\"firewall\"+'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The utilised CPU of a microservice running on a cluster\n",
    "import os\n",
    "def getchar(string, n):\n",
    "    return str(string)[n - 1]\n",
    "def cpuUtilised(cluster, app_name):\n",
    "    output = os.popen('kubectl top pod --context=' + cluster).read()\n",
    "    lines = output.split(\"\\n\")\n",
    "    which_app = len(app_name)\n",
    "    cpu = ''\n",
    "    cpu_u=0\n",
    "    limits=0\n",
    "    for line in lines[:-1]:\n",
    "        items = line.split()\n",
    "        if items[0][:which_app] == app_name:\n",
    "            if getchar(items[0], which_app) == 's':\n",
    "                cpu = items[1]\n",
    "                cpu = cpu[:-1]\n",
    "                cpu = int(cpu)\n",
    "                cpu_u=cpu_u + cpu\n",
    "            if getchar(items[0], which_app) == 'm':\n",
    "                cpu = items[1]\n",
    "                cpu = cpu[:-1]\n",
    "                cpu = int(cpu)\n",
    "                cpu_u=cpu_u + cpu\n",
    "            if getchar(items[0], which_app) == 'l':\n",
    "                cpu = items[1]\n",
    "                cpu = cpu[:-1]\n",
    "                cpu = int(cpu)\n",
    "                cpu_u=cpu_u + cpu\n",
    "\n",
    "        \n",
    "                \n",
    "#    print(limits - cpu_u)\n",
    "    return cpu_u\n",
    "\n",
    "\n",
    "cpuUtilised(\"cluster1-cntx\",\"firewall\"+'m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The available CPU of a microservice running on a cluster\n",
    "import os\n",
    "def getchar(string, n):\n",
    "    return str(string)[n - 1]\n",
    "def cpuAvail(cluster, app_name):\n",
    "    output = os.popen('kubectl top pod --context=' + cluster).read()\n",
    "    lines = output.split(\"\\n\")\n",
    "    which_app = len(app_name)\n",
    "    cpu = ''\n",
    "    cpu_u=0\n",
    "    limits=0\n",
    "    for line in lines[:-1]:\n",
    "        items = line.split()\n",
    "        if items[0][:which_app] == app_name:\n",
    "            if getchar(items[0], which_app) == 's':\n",
    "                limits = 600\n",
    "                cpu = items[1]\n",
    "                cpu = cpu[:-1]\n",
    "                cpu = int(cpu)\n",
    "                cpu_u=cpu_u + cpu\n",
    "            if getchar(items[0], which_app) == 'm':\n",
    "                limits = 5*600\n",
    "                cpu = items[1]\n",
    "                cpu = cpu[:-1]\n",
    "                cpu = int(cpu)\n",
    "                cpu_u=cpu_u + cpu\n",
    "            if getchar(items[0], which_app) == 'l':\n",
    "                limits = 10*600\n",
    "                cpu = items[1]\n",
    "                cpu = cpu[:-1]\n",
    "                cpu = int(cpu)\n",
    "                cpu_u=cpu_u + cpu\n",
    "\n",
    "        \n",
    "                \n",
    "#    print(limits - cpu_u)\n",
    "    return limits - cpu_u\n",
    "\n",
    "\n",
    "cpuAvail(\"cluster1-cntx\",\"firewall\"+'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Which microservice configuration is running: small, medium, or large\n",
    "def getchar(string, n):\n",
    "    return str(string)[n - 1]\n",
    "def whichConf(cluster, app_name):\n",
    "    size = []\n",
    "    output = os.popen('sudo kubectl get services --context=' + cluster).read()\n",
    "    lines = output.split(\"\\n\")\n",
    "    which_app = len(app_name)\n",
    "    for line in lines[:-1]:\n",
    "        items = line.split()\n",
    "        if items[0][:which_app] == app_name:\n",
    "            size.append(getchar(items[0], which_app+1))\n",
    "    #        print(size)\n",
    "    return size\n",
    "whichConf(\"cluster1-cntx\",\"firewall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function orders the running microservies on a cluster increasingly according to proccessing delay\n",
    "def orderService(cluster, app_name):\n",
    "    delay = []\n",
    "    size = whichConf(cluster, app_name)\n",
    "    for s in range(len(size)):\n",
    "        delay.append(bf[app_name][app_name+size[s]])\n",
    "    sorted_delay = [x for _,x in sorted(zip(delay,size))]\n",
    "    return sorted_delay\n",
    " #   print(delay)\n",
    "                \n",
    "    \n",
    "orderService(\"cluster1-cntx\",\"firewall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "def getIP(cluster, app_name):\n",
    "    output = os.popen('kubectl get service -o wide --context=' + cluster).read()\n",
    "    lines = output.split(\"\\n\")\n",
    "    serviceIP = ''\n",
    "    port=''\n",
    "    which_app = len(app_name)\n",
    "    for line in lines[:-1]:\n",
    "        items = line.split()\n",
    "        if items[0][:which_app] == app_name:\n",
    "            serviceIP=items[2]+\":\"+items[4][:4]\n",
    "    return serviceIP\n",
    "getIP(\"cluster1-cntx\",\"firewall\"+'m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cluster1-cntx', 'cluster2-cntx']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function orders the available links from source cluster increasingly according to latency\n",
    "def orderlink(cluster):\n",
    "    delay = []\n",
    "    cnx = []\n",
    "    for key in range(len(list(link_latency[\"cluster1-cntx\"].keys()))):\n",
    "        delay.append(link_latency[\"cluster1-cntx\"][list(link_latency[\"cluster1-cntx\"].keys())[key]])\n",
    "        cnx.append(list(link_latency[\"cluster1-cntx\"].keys())[key])\n",
    "    sorted_delay = [x for _,x in sorted(zip(delay,cnx))]\n",
    "    return sorted_delay\n",
    " #   print(delay)\n",
    "                \n",
    "    \n",
    "orderlink(\"cluster1-cntx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2842"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Available CPU on a cluster in millicores\n",
    "def clusterCPU(cluster):\n",
    "    cpu=0\n",
    "    cpu_cl=0\n",
    "    nodes=0\n",
    "    output = os.popen('sudo kubectl top nodes --context=' + cluster).read()\n",
    "    lines = output.split(\"\\n\")\n",
    "    for line in lines[:-1]:\n",
    "        items = line.split()\n",
    "        if items[0] != \"NAME\":\n",
    "            cpu = items[1]\n",
    "            cpu = cpu[:-1]\n",
    "            cpu = int(cpu)\n",
    "            cpu_cl = cpu_cl + cpu\n",
    "            nodes=nodes+1\n",
    "    return nodes*2000 - cpu_cl\n",
    "\n",
    "clusterCPU('cluster1-cntx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Place and Assign new mircroservice to requests</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<h1>Place and Assign new mircroservice to requests</h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cluster1-cntx', 'cluster1-cntx', 'cluster1-cntx', 'cluster2-cntx']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns the lists of clusters on which a microservice type is running\n",
    "def checkdeploy(app_name):\n",
    "    cl = 0\n",
    "    p=0\n",
    "    cluster=[]\n",
    "    while p == 0:\n",
    "        for cl in range(len(G)):\n",
    "            i = list(G.keys())[cl]\n",
    "            output = os.popen('sudo kubectl get services --context=' +i).read()\n",
    "            lines = output.split(\"\\n\")\n",
    "            which_app = len(app_name)\n",
    "            for line in lines[:-1]:\n",
    "                items = line.split()\n",
    "                if items[0][:which_app] == app_name:\n",
    "                    cluster.append(i)\n",
    "                    p=1\n",
    "                    \n",
    "    return cluster\n",
    "checkdeploy(\"proxy\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignService(i, q, usecase_s):\n",
    "#    req = 1\n",
    "#    cl = 1\n",
    "    config_idx='' \n",
    "    size=[]\n",
    "    avail=0\n",
    "    cpu_q= usecase_s[u][q]\n",
    "    r = runningPods(i, q)\n",
    "    if r >= 1: #there's a function already running on this cluster \n",
    "        size = orderService(i,q) #which function size is running? order by delay\n",
    "        for s in range(len(size)): \n",
    "            avail = cpuAvail(i, q+size[s]) #the available CPU depends on the service configuration\n",
    "            if avail > cpu_q: #if the available resources > profiled resources requested \n",
    "                config_idx = size[s] #then get IP to assign the service to proxy\n",
    "                break\n",
    "    elif r ==0 : #no service running \n",
    "        config_idx=''          \n",
    "    #print(proxy_conf)\n",
    "    return config_idx\n",
    "\n",
    "def newService(i,q, usecase_s):\n",
    "    config_idx=''\n",
    "    cpu_q= usecase_s[u][q]\n",
    "    r = runningPods(i, q)\n",
    "    if 100 >= cpu_q and runningPods(i, q+'s') == 0 and 200 < clusterCPU(i): #deploy new small service\n",
    "        #place(i,q,'s')\n",
    "        config_idx='s'       \n",
    "    elif 200 >= cpu_q and runningPods(i, q+'m')==0 and 300 < clusterCPU(i): #deploy new medium service\n",
    "        config_idx='m'\n",
    "    elif 500 >= cpu_q and runningPods(i, q+'l')==0 and 600 < clusterCPU(i): #deploy new large service\n",
    "        config_idx='l'\n",
    "\n",
    "  #  elif proxy_conf=='':\n",
    "  #      print(\"placement failed\")\n",
    "        \n",
    "    \n",
    "    #print(proxy_conf)\n",
    "    return config_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Monitor and delete function</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<h1>Monitor and delete function</h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def monitorUsage(t): #This function is terminate services once idle for a time \n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "def deletePod(app_name, timer, cluster):\n",
    "    #print(\"thread is starting\")\n",
    "    action=[None] * 5\n",
    "    policy=[]\n",
    "    cpu = 0\n",
    "    timeout = time.time() + timer\n",
    "    while cpu == 0:\n",
    "        cpu= cpuUtilised(cluster, app_name)\n",
    "        if time.time() > timeout:\n",
    "            action[0]=cluster\n",
    "            action[1]=0\n",
    "            action[2]= app_name[len(app_name)-1]\n",
    "            action[3]= app_name[:-1]\n",
    "            action[4]= 0\n",
    "            policy.append(action)\n",
    "            break\n",
    "    return policy\n",
    "class ThreadWithResult(threading.Thread):\n",
    "    def __init__(self, group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None):\n",
    "        def function():\n",
    "            self.result = target(*args, **kwargs)\n",
    "        super().__init__(group=group, target=function, name=name, daemon=daemon)\n",
    "\n",
    "        \n",
    "def monitorUsage(cluster, timer):\n",
    "\n",
    "    for service in range(len(bf)):\n",
    "        threads = list()\n",
    "        ser = list(bf.keys())[service]\n",
    "        s = whichConf(cluster,ser)\n",
    "        if s != []:\n",
    "            for size in range(len(s)):\n",
    "                app_name=ser+s[size]\n",
    "                x =ThreadWithResult(target=deletePod, args=(app_name, timer, cluster,))\n",
    "                x.daemon = True\n",
    "                threads.append(x)\n",
    "                x.start()\n",
    "            for x in threads:\n",
    "                x.join()\n",
    "                print(x.result)\n",
    "                \n",
    "\n",
    "        \n",
    "#monitorUsage(\"cluster1-cntx\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With new requests you need to initialise and empty the these values again\n",
    "#Run this once with every new G_req\n",
    "\n",
    "m_ser = []\n",
    "m_cl= ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateAssign(a_ser):\n",
    "    m_ser.append(a_ser)\n",
    "    return  m_ser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(m_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cluster1-cntx', 2, 's', 'encrypt', 'proxy1']]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this feeds the RL agent with heuristic policies \n",
    "#[cluster-idx, action-idx, config-idx, app-idx, proxy-idx]\n",
    "\n",
    "def main():\n",
    "\n",
    "    proxy_idx=proxy\n",
    "    action=[None] * 5\n",
    "    policy=[]\n",
    "    global m_cl\n",
    "    global m_ser\n",
    "\n",
    "\n",
    "    startingCluster = checkdeploy(proxy_idx)\n",
    "    OG = orderlink(startingCluster) #try the fastest link first\n",
    "    for cl in range(len(OG)): #go over clusters \n",
    "        i = list(G.keys())[cl]\n",
    "        for req in range(len(G_req)):\n",
    "            q = list(G_req.keys())[req] #go over functions to deploy\n",
    "            if q not in m_ser:\n",
    "                if q == list(G_req.keys())[0]: #The first function can be deployed on any cluster\n",
    "                    if assignService(i,q, usecase) != \"\": #there is atleast one good candidate service running on i \n",
    "                        action[0]=i\n",
    "                        action[1]=1\n",
    "                        action[2]= assignService(i,q, usecase)\n",
    "                        action[3]= q\n",
    "                        action[4]= proxy_idx\n",
    "                        policy.append(action)\n",
    "                        m_cl= i #to check connection with next deployment\n",
    "                        #m_serv=m_ser.append(q)\n",
    "                        updateAssign(q)\n",
    "                        #print(m_ser)\n",
    "                        return policy\n",
    "                elif m_cl in G[i]: #connection with previous bf and not already deployed\n",
    "                    if assignService(i,q, usecase) != \"\": #can be deployed\n",
    "                        action[0]=i\n",
    "                        action[1]=1\n",
    "                        action[2]= assignService(i,q, usecase)\n",
    "                        action[3]= q\n",
    "                        action[4]= proxy_idx\n",
    "                        policy.append(action)\n",
    "                        m_cl= i\n",
    "                        #m_ser=m_ser.append(q)\n",
    "                        updateAssign(q)\n",
    "                        #print(m_ser)\n",
    "                        return policy\n",
    "            else: \n",
    "                continue\n",
    "        if len(m_ser) < len(G_req): #not all deployed yet we try a different cluster      \n",
    "            continue \n",
    "        else:\n",
    "            break\n",
    "    if len(m_ser) < len(G_req): #not all functions could be deployed \n",
    "        for cl in range(len(OG)): #go over clusters \n",
    "            i = list(G.keys())[cl]\n",
    "            for req in range(len(G_req)):\n",
    "                q = list(G_req.keys())[req] #go over functions to deploy\n",
    "                if q not in m_ser:\n",
    "                    if q == list(G_req.keys())[0]:\n",
    "                        if newService(i,q, usecase) != '':\n",
    "                            action[0]=i\n",
    "                            action[1]=2\n",
    "                            action[2]= newService(i,q, usecase)\n",
    "                            action[3]= q\n",
    "                            action[4]= proxy_idx\n",
    "                            policy.append(action)\n",
    "                            m_cl= i\n",
    "                            #m_ser=m_ser.append(q)\n",
    "                            updateAssign(q)\n",
    "                            #print(m_ser)\n",
    "                            return policy\n",
    "                    elif m_cl in G[i]: #connection with previous bf and not already deployed\n",
    "                        if newService(i,q, usecase) != '':\n",
    "                            action[0]=i\n",
    "                            action[1]=2\n",
    "                            action[2]= newService(i,q, usecase)\n",
    "                            action[3]= q\n",
    "                            action[4]= proxy_idx\n",
    "                            policy.append(action)\n",
    "                            m_cl= i\n",
    "                            #m_ser=m_ser.append(q)\n",
    "                            updateAssign(q)\n",
    "                            #print(m_ser)\n",
    "                            return policy\n",
    "            if len(m_ser) < len(G_req): #not all deployed yet we try a different cluster      \n",
    "                continue \n",
    "            else:\n",
    "                break\n",
    "    #proxyConfig(IPs, proxy)\n",
    "    #print(IPs)\n",
    "    #print(m_ser)\n",
    "    \n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Deploy and monitor microservices</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"<h1>Deploy and monitor microservices</h1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cluster1-cntx', 2, 's', 'firewall', 'proxy']]\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#def deploy(proxy, G_req):\n",
    "#    for r in range(len(G_req)):\n",
    "#        main(proxy, G_req)\n",
    "#        time.sleep(1)\n",
    "\n",
    "def heuristic():\n",
    "    threads = list()\n",
    "    x =ThreadWithResult(target=main, args=(\"proxy\", G_req, ))\n",
    "    x.daemon = True\n",
    "    x.start()\n",
    "    threads.append(x)\n",
    "    for cl in range(len(G)): #go over clusters \n",
    "        i = list(G.keys())[cl]\n",
    "        x = ThreadWithResult(target=monitorUsage, args=(i,10000, ))\n",
    "        x.daemon = True\n",
    "        threads.append(x)\n",
    "        x.start()\n",
    "    for x in threads:\n",
    "        x.join()\n",
    "        print(x.result)\n",
    "        \n",
    "heuristic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
